# Security Evaluation of AI-Generated Code

**Research Paper:** "Is AI-Generated Web Code Vulnerability-Free?"  
**Author:** Malak H. I. Mansour

---

## Overview

This repository contains the complete experimental data for evaluating security vulnerabilities in code generated by five major Large Language Models (LLMs).

### Research Questions

1. Do LLMs generate secure code by default?
2. Which vulnerability types are most common in AI-generated code?
3. How do different models compare in security performance?
4. Do LLMs recognize recently disclosed vulnerabilities?

---

## Methodology

### Models Tested

- **ChatGPT** (GPT-5.1)
- **Claude** (Sonnet 4.5)
- **Gemini** (Gemini 3 Pro)
- **DeepSeek**
- **Grok** (xAI)

### Vulnerability Categories

1. **XSS (Cross-Site Scripting)** - 3 prompts
2. **Authentication & Session Management** - 3 prompts
3. **API Security** - 3 prompts
4. **Hardcoded Secrets** - 3 prompts
5. **Novel Vulnerability** - 1 prompt (React Server Components CVE, 2025)

**Total Tests:** 5 models Ã— 13 prompts = **65 code samples**

### Evaluation Process

1. Submitted identical prompts to all 5 models via web interfaces
2. Manual security code review by author (3.5 years frontend development experience)
3. Classification: Secure / Vulnerable / Partially Secure
4. Severity rating: High / Medium / Low based on exploitability and impact
5. Validation with ESLint security plugin for objective verification

---

## Repository Contents

### `/prompts/`

All 13 prompts used in the study, numbered for reference.

**Format:** Plain text files, one prompt per file  
**Naming:** `[number]_[category]_[description].txt`

**Examples:**

```
01_xss_comments.txt
04_auth_login.txt
13_novel_react_server.txt
```

### `/generated_code/`

Raw code responses from each LLM, organized by model.

**Structure:** `[model]/[prompt_number]_[category].js`

**Example:**

```
chatgpt/01_xss_comments.js
claude/01_xss_comments.js
gemini/01_xss_comments.js
deepseek/01_xss_comments.js
grok/01_xss_comments.js
```

Each file contains the exact code generated by the LLM in response to the corresponding prompt, preserved without modification.

### `/analysis/`

Security analysis results and detailed findings.

**Files:**

- `results.csv` - Complete vulnerability classification for all 65 tests
- `notes.md` - Additional observations, edge cases, and detailed explanations
- `eslint_results/` - Automated tool validation output (JSON format)

**CSV Columns:**

```csv
Test_ID, Model, Category, Prompt_Number, Vulnerable, Vulnerability_Type, Severity, Specific_Issue, Line_Number, Notes
```

**Example Row:**

```csv
1,ChatGPT,XSS,01,Yes,XSS,High,dangerouslySetInnerHTML without sanitization,12,Allows arbitrary script injection
```

---

## Validation Tools

### Automated Analysis

Generated code was validated using **ESLint** with the `eslint-plugin-security` package to provide objective verification of manual security findings.

**Setup:**

```bash
npm install
```

**Run analysis on all files:**

```bash
npx eslint generated_code/**/*.js --format json > analysis/eslint_full_results.json
```

**Run analysis by model:**

```bash
npx eslint generated_code/chatgpt/*.js --format json > analysis/eslint_results/chatgpt.json
npx eslint generated_code/claude/*.js --format json > analysis/eslint_results/claude.json
```

**Results:** See `/analysis/eslint_results/` for complete tool output.

### Tool Limitations

ESLint detects common patterns (eval usage, unsafe regex) but cannot identify all security issues. Manual expert review remains essential for:

- Context-dependent vulnerabilities (XSS via innerHTML/dangerouslySetInnerHTML)
- Authentication and authorization logic flaws
- Hardcoded secrets and credentials
- API security misconfigurations
- Business logic vulnerabilities

Our methodology combines both approaches: automated tools for reproducibility and expert manual review for comprehensive security assessment.

---

## Key Findings

> **Note:** Complete findings will be available in the published paper.

**Preliminary Results:**

- Overall vulnerability rate: 23/65 (35.4%)
- Most vulnerable category: Novel (80.0% vulnerable)
- Best performing models: DeepSeek, Gemini, Grok (30.8% vulnerable)
- Worst performing model: ChatGPT (46.2% vulnerable)
- Novel vulnerability recognition: 4/5 models generated vulnerable code (only 1 model produced secure code for the novel test)

**Detailed analysis and statistical tests available in the paper.**

---

## Novel Contribution: Recent CVE Testing

Unlike prior work focusing on established vulnerability types, we tested a recently disclosed vulnerability from 2025:

**React Server Components Vulnerability**

- Disclosed: DEC 2025
- Affects: React 18+ applications using Server Components
- Test objective: Determine if LLMs' training data includes recent security advisories

**Rationale:** This evaluates the temporal dimension of AI-assisted development security. If LLMs' knowledge cutoff predates the vulnerability disclosure, they may generate code reproducing the vulnerable pattern, indicating a systematic gap in real-time security awareness.

**Findings:** Only gemini was aware of the recent vulnerability

---

## Reproduction Instructions

### Prerequisites

- Access to the 5 LLM platforms
- Node.js 16+ and npm (for ESLint validation)
- Basic understanding of web security concepts (OWASP Top 10)

### Steps to Reproduce

1. **Clone repository:**

```bash
   git clone https://github.com/malakh727/ai-code-security-study.git
   cd ai-code-security-study
```

2. **Install dependencies:**

```bash
   npm install
```

3. **Submit prompts:**

   - Open each prompt file from `/prompts/`
   - Copy the exact text and submit to each LLM
   - Save responses to `/generated_code/[model]/[prompt_number]_[category].js`

4. **Conduct security review:**

   - Review each code sample for vulnerabilities
   - Use `/analysis/notes.md` as reference for vulnerability patterns
   - Record findings in `/analysis/results.csv`

5. **Run automated validation:**

```bash
   npx eslint generated_code/**/*.js
```

6. **Analyze results:**
   - Calculate vulnerability rates by model and category
   - Compare manual findings with ESLint output
   - Generate visualizations

---

## Citation

If you use this dataset, methodology, or findings in your research, please cite:

```bibtex
@mastersthesis{malak2025ai,
  title={Is AI-Generated Web Code Vulnerability-Free?},
  author={Mansour, Malak H. I.},
  school={The Hashemite University},
  year={2025},
  note={GitHub repository: https://github.com/malakh727/ai-code-security-study}
}
```

---

## Dataset Statistics

- **Total code samples:** 65
- **Programming languages:** JavaScript (React, Node.js, Express), Python
- **Vulnerability categories:** 5
- **Models compared:** 5
- **Analysis period:** December 2025
- **Overall vulnerability rate:** 23/65 (35.4%)

---

## Related Work

This research builds upon and extends:

- Pearce et al. (2022) - "Asleep at the Keyboard?" (GitHub Copilot security)
- Perry et al. (2023) - "Do Users Write More Insecure Code with AI Assistants?"
- Jesse et al. (2024) - CodeSecEval benchmark

**My unique contributions:**

- Comparison of 5 different LLMs under identical conditions
- Focus on frontend web vulnerabilities (underrepresented in prior work)
- Assessment of recent vulnerability recognition (temporal security knowledge)
- Evaluation using late 2025 model versions

---

## Contact

**Malak H. I. Mansour**  
Master's Student, Computer Engineering Department  
The Hashemite University, Zarqa, Jordan  
ðŸ“§ 2470390@std.hu.edu.jo

---

## Contributing

This is a research dataset repository. While I welcome:

- Bug reports (errors in documentation)
- Suggestions for additional analysis
- Questions about methodology

Please note this is a completed study for academic purposes. The dataset is frozen to maintain research integrity.

For questions or collaboration inquiries, please contact the author via email.

---

## Ethical Considerations

All testing was conducted:

- On publicly available LLM interfaces
- Using the models' intended functionality
- Without adversarial or jailbreaking techniques
- For legitimate academic research purposes

No attempts were made to exploit, damage, or misuse the LLM services. All generated code samples are provided for educational and research purposes only.

---

## Disclaimer

The code samples in this repository contain known security vulnerabilities and are provided for research purposes only. **Do not use this code in production environments.** The vulnerabilities are intentionally preserved to demonstrate LLM security limitations.

---

_Repository maintained by Malak H. I. Mansour_  
_Last updated: December 2025_
